# ICCV2023-Accepted-Papers

# Day 3: Wednesday 4th 09:30-10:30 - Orals
## Session: 3D from multi-view and sensors 

| **Paper Title** | **ArXiv** | **Code** | **Project page** | **Team** |
|-----------|:--------:|:---------:|:---------:|:---------:|
| Multi-Modal Neural Radiance Field for Monocular Dense SLAM with a Light-Weight ToF Sensor | [Arxiv](https://arxiv.org/abs/2308.14383) | [Code](https://github.com/zju3dv/tof_slam) | [Project page](https://zju3dv.github.io/tof_slam/) | Team: State Key Lab of CAD & CG, Zhejiang University ([link](http://www.cad.zju.edu.cn/english.html)) | 
| ScanNet++: A High-Fidelity Dataset of 3D Indoor Scenes | [Arxiv](https://arxiv.org/abs/2308.11417) | [Code](https://cy94.github.io/scannetpp/#code-toolkit) | [Project page](https://cy94.github.io/scannetpp/) | Team: 3D AI Lab, Technical University of Munich ([link](https://www.3dunderstanding.org/index.html)) | 
| Translating Images to Road Network: A Non-Autoregressive Sequence-to-Sequence Approach | (Comming) | (Comming) | (Comming) | Team: Zhang-vsion Group, Fudan University ([link](https://fudan-zvg.github.io)) | 
| Doppelgangers: Learning to Disambiguate Images of Similar Structures | [Arxiv](https://arxiv.org/pdf/2309.02420.pdf) | [Code](https://github.com/RuojinCai/Doppelgangers) | [Project page](https://doppelgangers-3d.github.io) | Team: (Prof. Bharath Hariharan, Prof. Noah Snavely [link](http://home.bharathh.info/) / [link2](https://www.cs.cornell.edu/~snavely/)) | 
 | EgoLoc: Revisiting 3D Object Localization from Egocentric Videos with Visual Queries | [Arxiv](https://arxiv.org/abs/2212.06969) | [Code](https://github.com/Wayne-Mai/EgoLoc) | N/A | Team: Image and Video Understanding Laboratory (IVUL), King Abdullah University of Science and Technology (KAUST)([link](https://cemse.kaust.edu.sa/ivul)) | 
 | ClothPose: A Real-world Benchmark for Visual Analysis of Garment Pose via An Indirect Recording Solution | (Comming) | (Comming) | (Comming) | Team: Shanghai Jiao Tong University([link(Comming)]()) | 
 | EMR-MSF: Self-Supervised Recurrent Monocular Scene Flow Exploiting Ego-Motion Rigidity | [Arxiv](https://arxiv.org/abs/2309.01296) | (Comming) | (Comming) | Team: Okutomi & Tanaka Lab, Tokyo Institute of Technology ([link](http://www.ok.sc.e.titech.ac.jp/mem/member.shtml)) | 
 | ENVIDR: Implicit Differentiable Renderer with Neural Environment Lighting | [Arxiv](https://arxiv.org/abs/2303.13022) | [Code](https://github.com/nexuslrf/ENVIDR) | [Project page](https://nexuslrf.github.io/ENVIDR/) | Team: EMBARC RESEARCH GROUP, University of Toronto ([link](https://www.cs.toronto.edu/~nandita/index.html)) | 

# Day 3: Wednesday 4th 10:30-12:30 - Posters
## Session: Vision and robotics
| **Paper Title** | **ArXiv** | **Code** | **Project page** | **Team** |
|-----------|:--------:|:---------:|:---------:|:---------:|
 | Simoun: Synergizing Interactive Motion-appearance Understanding for Vision-based Reinforcement Learning | (Comming) | (Comming) | (Comming) | Team: Multimedia Learning Group, Peking University ([link](https://www.pkuml.org/staff/all-staff.html)) | 
 | Among Us: Adversarially Robust Collaborative Perception by Consensus | [Arxiv](https://arxiv.org/abs/2303.09495) | [Code](https://github.com/coperception/ROBOSAC) | N/A | Team:  ([link]()) |
| Walking Your LiDOG: A Journey Through Multiple Domains for LiDAR Semantic Segmentation | [Arxiv](https://arxiv.org/abs/2304.11705) | [Code](https://github.com/saltoricristiano/LiDOG) | N/A | Team: University of Trento ([link]()) |



